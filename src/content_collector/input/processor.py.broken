"""Input file processing for URL extraction."""

import csv
from pathlib import Path
from typing import List, Set
from urllib.parse import urlparse

import structlog
from pydantic import BaseModel, HttpUrl, field_validator

logger = structlog.get_logger()


class URLEntry(BaseModel):
    """Model for URL entry with validation."""
    
    url: HttpUrl
    source_file: str
    line_number: int
    
    @field_validator('url')
    @classmethod
    def validate_url_scheme(cls, v: HttpUrl) -> HttpUrl:
        """Ensure URL has http or https scheme."""
        if str(v).startswith(('http://', 'https://')):
            return v
        raise ValueError('URL must start with http:// or https://')


class InputProcessor:
    """Processes input files to extract and validate URLs."""
    
    def __init__(self) -> None:
        """Initialize input processor."""
        self.logger = logger.bind(component="input_processor")
    
    async def process_input_file(self, input_file: Path) -> List[URLEntry]:
        """
        Process input file containing CSV file paths.
        
        Args:
            input_file: Path to .txt file containing CSV file paths
            
        Returns:
            List of validated URL entries
        """
        Process input file and return list of URL entries.
        
        Args:
            input_file: Path to input file (either CSV file or file containing CSV paths)
            
        Returns:
            List of URLEntry objects
        """
        self.logger.info("Processing input file", file=str(input_file))
        
        if not input_file.exists():
            raise FileNotFoundError(f"Input file not found: {input_file}")
        
        # Check if input file is a CSV file itself
        if input_file.suffix.lower() == '.csv':
            # Process as direct CSV file
            self.logger.debug("Processing as direct CSV file")
            urls = await self._process_csv_file(input_file)
        else:
            # Process as file containing CSV paths
            self.logger.debug("Processing as file containing CSV paths")
            csv_files = self._read_csv_paths(input_file)
            urls = []
            
            for csv_file in csv_files:
                file_urls = await self._process_csv_file(csv_file)
                urls.extend(file_urls)
        
        # Deduplicate URLs while preserving first occurrence
        unique_urls = self._deduplicate_urls(urls)
        
        csv_file_count = 1 if input_file.suffix.lower() == '.csv' else len(self._read_csv_paths(input_file))
        
        self.logger.info(
            "Input processing completed",
            total_urls=len(unique_urls),
            csv_files=csv_file_count
        )
        
        return unique_urls
    
    def _read_csv_paths(self, input_file: Path) -> List[Path]:
        """Read CSV file paths from input file."""
        csv_files = []
        
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#'):  # Skip empty lines and comments
                    csv_path = Path(line)
                    if not csv_path.is_absolute():
                        # Resolve relative to input file directory
                        csv_path = input_file.parent / csv_path
                    
                    if csv_path.exists():
                        csv_files.append(csv_path)
                    else:
                        self.logger.warning(
                            "CSV file not found",
                            file=str(csv_path),
                            line=line_num
                        )
        
        return csv_files
    
    async def _process_csv_file(self, csv_file: Path) -> List[URLEntry]:
        """Process individual CSV file to extract URLs."""
        self.logger.debug("Processing CSV file", file=str(csv_file))
        
        urls = []
        
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                content = f.read()
                f.seek(0)
                
                # Try to detect delimiter and header
                try:
                    sniffer = csv.Sniffer()
                    delimiter = sniffer.sniff(content[:1024]).delimiter
                    has_header = sniffer.has_header(content[:1024])
                except csv.Error:
                    # Default to comma delimiter if detection fails
                    delimiter = ','
                    # Check if first line looks like a header
                    first_line = content.split('\n')[0] if content else ''
                    has_header = 'url' in first_line.lower()
                
                reader = csv.reader(f, delimiter=delimiter)
                
                if has_header:
                    headers = next(reader, None)  # Skip header row
                    self.logger.debug("CSV headers detected", headers=headers)
                
                for line_num, row in enumerate(reader, 1):
                    if row and len(row) > 0:  # Skip empty rows
                        url_string = row[0].strip()  # Take first column as URL
                        
                        # Skip if it looks like a header that wasn't detected
                        if url_string.lower() in ['url', 'link', 'website']:
                            continue
                        
                        try:
                            url_entry = URLEntry(
                                url=url_string,
                                source_file=str(csv_file),
                                line_number=line_num + (1 if has_header else 0)
                            )
                            urls.append(url_entry)
                        except Exception as e:
                            self.logger.warning(
                                "Invalid URL skipped",
                                url=url_string,
                                file=str(csv_file),
                                line=line_num + (1 if has_header else 0),
                                error=str(e)
                            )
        
        except Exception as e:
            self.logger.error(
                "Failed to process CSV file",
                file=str(csv_file),
                error=str(e)
            )
        
        self.logger.debug(
            "CSV file processed",
            file=str(csv_file),
            urls_found=len(urls)
        )
        
        return urls
    
    def _deduplicate_urls(self, urls: List[URLEntry]) -> List[URLEntry]:
        """Remove duplicate URLs while preserving first occurrence."""
        seen_urls: Set[str] = set()
        unique_urls = []
        
        for url_entry in urls:
            url_str = str(url_entry.url)
            if url_str not in seen_urls:
                seen_urls.add(url_str)
                unique_urls.append(url_entry)
        
        duplicate_count = len(urls) - len(unique_urls)
        if duplicate_count > 0:
            self.logger.info(
                "Duplicate URLs removed",
                duplicates=duplicate_count,
                unique_urls=len(unique_urls)
            )
        
        return unique_urls
